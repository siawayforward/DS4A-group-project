{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAQ Data\n",
    "## Part 1: Compiling Data using the OpenAQ api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can pull the data from the past 2 years from https://docs.openaq.org/\n",
    "\n",
    "# this is a link for a map showing sources and readings: https://openaq.org/#/map?&_k=tqphyo\n",
    "# you can get more details for any location (and download data)\n",
    "# san fran (San Francisco-Oakland-Fremont) data: https://openaq.org/#/location/San%20Francisco?_k=9pwelq\n",
    "# 65k records from 07/2018 - today\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for archived data (06/2015 - 04/2018), you can use https://openaq-data.s3.amazonaws.com/index.html\n",
    "# *about 03/2016 for san fran data\n",
    "# for each city there is a record for each type of reading (o3, co, pm, etc.) noted by the key 'parameter'\n",
    "# the source of the reading is noted by the key 'attribution', which is a dictionary with the name and url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file openAQ already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a folder to save the data\n",
    "! mkdir openAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, text):\n",
    "    folder = 'openAQ'\n",
    "    extension = 'csv'\n",
    "    path = '{}\\{}.{}'.format(folder, filename, extension)\n",
    "    file = open(path, 'w')\n",
    "    file.write(text)\n",
    "    file.close\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_for_range(start_date, end_date):\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    dates = [str(timestamp.date()) for timestamp in date_range]\n",
    "    return dates\n",
    "\n",
    "def is_keywords_in_string(string, keywords):\n",
    "    has_keyword = False\n",
    "    for keyword in keywords:\n",
    "        has_keyword = keyword in string\n",
    "        if has_keyword:\n",
    "            return has_keyword\n",
    "    return has_keyword\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for handling requests\n",
    "def request_openAQ_data(date):\n",
    "    url = 'https://openaq-data.s3.amazonaws.com/{}.csv'.format(date)\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        return response\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes in a date as a string with the format yyyy-mm-dd\n",
    "# filter the results based on lines containing keywords\n",
    "def filter_date_data_for_keywords(date, keywords):\n",
    "    filtered_data = ''\n",
    "    response = request_openAQ_data(date)\n",
    "    if response is not None:\n",
    "        for line in response.iter_lines():\n",
    "            line = str(line, 'utf-8')\n",
    "            if is_keywords_in_string(line, keywords):\n",
    "                filtered_data += line + '\\n' # probably could optimize\n",
    "    return filtered_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_in_date_range(start_date, end_date, keywords):\n",
    "    dates = get_dates_for_range(start_date, end_date)\n",
    "    for date in dates:\n",
    "        print(date)\n",
    "        data = filter_date_data_for_keywords(date, keywords)\n",
    "        if data:\n",
    "            save_file(date, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull all of the daily data and filter the results for san-fran related records and save to a new csv\n",
    "start_date = '2015-06-09'\n",
    "end_date = '2018-04-06'\n",
    "keywords = ['San Fran']\n",
    "filter_data_in_date_range(start_date, end_date, keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Details\n",
    "\n",
    "1. location: The name of the location of the site where the reading was observed\n",
    "2. city: The name of the city\n",
    "3. country: The name of the country\n",
    "4. utc: The time of the reading using the UTC timezone\n",
    "5. local: The time of the reading using the Local timezone\n",
    "6. parameter: The name of the pollutant being observed\n",
    "7. value: The value of the pollutant reading\n",
    "8. unit: The units of the pollutant read\n",
    "9. latitude: The latitude of the site\n",
    "10. longitude: The longitude of the site\n",
    "11. attribution: A dictionary containing the name and url of the source of this data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "This API also provides fairly clean data, with a high degree of geographic specificity. This data will provide the foundation for our analysis, allowing us to determine the severity of fire-related air pollution over time in the Bay Area and thereby to examine its relationship to travel- and mobility-related factors. There is archived daily data from 2016 - 2018. Each file is 270 KB with a total of 200 MB. More recent data (up to 2 years) can be requested with their api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# getting a list of US cities available from OpenAQ\n",
    "cities = requests.get('https://api.openaq.org/v1/cities?country=US&limit=1000')\n",
    "print(cities.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this list, we find that the areas of interest (in the Bay Area) are 'San Francisco-Oakland-Fremont', 'San Jose-Sunnyvale-Santa Clara', 'Vallejo-Fairfield', 'Napa', and 'Sonoma'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we next need to find out which 'locations' are included in each of these broader areas\n",
    "\n",
    "areas_of_interest = [\n",
    "    'San Francisco-Oakland-Fremont',\n",
    "    'San Jose-Sunnyvale-Santa Clara',\n",
    "    'Vallejo-Fairfield',\n",
    "    'Napa',\n",
    "    'Sonoma'\n",
    "]\n",
    "\n",
    "bay_area_locations = []\n",
    "\n",
    "# getting info for each area and adding the locations that it contains to a list\n",
    "for area in areas_of_interest:\n",
    "    parameters = { 'city[]': area }\n",
    "    response = requests.get('https://api.openaq.org/v1/locations', parameters)\n",
    "    \n",
    "    for result in response.json()['results']:\n",
    "        bay_area_locations.append(result['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_area_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# This is the recent data request\n",
    "# getting a sense of what the data for each location will look like... \n",
    "# testing the process on data for Alameda/Berkeley Aquatic Par location\n",
    "# focusing on PM2.5 because that's the main pollutant used to gauge wildfires' effects on air quality\n",
    "\n",
    "test_params = {\n",
    "    'city': 'ALAMEDA',\n",
    "    'location': 'Berkeley Aquatic Par',\n",
    "    'parameter': 'pm25',\n",
    "    'date_from': '2020-01-01',\n",
    "    'limit': 10\n",
    "}\n",
    "\n",
    "alameda_resp = requests.get('https://api.openaq.org/v1/measurements', test_params)\n",
    "print(alameda_resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alameda_resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>parameter</th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "      <th>unit</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berkeley Aquatic Par</td>\n",
       "      <td>pm25</td>\n",
       "      <td>{'utc': '2020-12-04T03:00:00Z', 'local': '2020...</td>\n",
       "      <td>30</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>{'latitude': 37.864767, 'longitude': -122.302741}</td>\n",
       "      <td>US</td>\n",
       "      <td>ALAMEDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Berkeley Aquatic Par</td>\n",
       "      <td>pm25</td>\n",
       "      <td>{'utc': '2020-12-04T02:00:00Z', 'local': '2020...</td>\n",
       "      <td>32</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>{'latitude': 37.864767, 'longitude': -122.302741}</td>\n",
       "      <td>US</td>\n",
       "      <td>ALAMEDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Berkeley Aquatic Par</td>\n",
       "      <td>pm25</td>\n",
       "      <td>{'utc': '2020-12-04T01:00:00Z', 'local': '2020...</td>\n",
       "      <td>39</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>{'latitude': 37.864767, 'longitude': -122.302741}</td>\n",
       "      <td>US</td>\n",
       "      <td>ALAMEDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Berkeley Aquatic Par</td>\n",
       "      <td>pm25</td>\n",
       "      <td>{'utc': '2020-12-04T00:00:00Z', 'local': '2020...</td>\n",
       "      <td>35</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>{'latitude': 37.864767, 'longitude': -122.302741}</td>\n",
       "      <td>US</td>\n",
       "      <td>ALAMEDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Berkeley Aquatic Par</td>\n",
       "      <td>pm25</td>\n",
       "      <td>{'utc': '2020-12-03T23:00:00Z', 'local': '2020...</td>\n",
       "      <td>36</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>{'latitude': 37.864767, 'longitude': -122.302741}</td>\n",
       "      <td>US</td>\n",
       "      <td>ALAMEDA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               location parameter  \\\n",
       "0  Berkeley Aquatic Par      pm25   \n",
       "1  Berkeley Aquatic Par      pm25   \n",
       "2  Berkeley Aquatic Par      pm25   \n",
       "3  Berkeley Aquatic Par      pm25   \n",
       "4  Berkeley Aquatic Par      pm25   \n",
       "\n",
       "                                                date  value   unit  \\\n",
       "0  {'utc': '2020-12-04T03:00:00Z', 'local': '2020...     30  µg/m³   \n",
       "1  {'utc': '2020-12-04T02:00:00Z', 'local': '2020...     32  µg/m³   \n",
       "2  {'utc': '2020-12-04T01:00:00Z', 'local': '2020...     39  µg/m³   \n",
       "3  {'utc': '2020-12-04T00:00:00Z', 'local': '2020...     35  µg/m³   \n",
       "4  {'utc': '2020-12-03T23:00:00Z', 'local': '2020...     36  µg/m³   \n",
       "\n",
       "                                         coordinates country     city  \n",
       "0  {'latitude': 37.864767, 'longitude': -122.302741}      US  ALAMEDA  \n",
       "1  {'latitude': 37.864767, 'longitude': -122.302741}      US  ALAMEDA  \n",
       "2  {'latitude': 37.864767, 'longitude': -122.302741}      US  ALAMEDA  \n",
       "3  {'latitude': 37.864767, 'longitude': -122.302741}      US  ALAMEDA  \n",
       "4  {'latitude': 37.864767, 'longitude': -122.302741}      US  ALAMEDA  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turning the alameda_resp dictionary into a df\n",
    "\n",
    "alameda_df = pd.DataFrame.from_dict(alameda_resp.json()['results'])\n",
    "alameda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berkeley Aquatic Par : 6462\n",
      "Bethel Island : 0\n",
      "Concord : 6637\n",
      "Hayward : 0\n",
      "Laney College : 6837\n",
      "Livermore - Rincon : 6863\n",
      "Oakland : 6811\n",
      "Oakland West : 6335\n",
      "Patterson Pass : 0\n",
      "Pleasanton - Owens C : 6675\n",
      "Redwood City : 6667\n",
      "Richmond - 7th St : 0\n",
      "San Francisco : 6467\n",
      "San Pablo - Rumrill : 6479\n",
      "San Rafael : 6670\n",
      "San Ramon : 0\n",
      "Gilory - 9th Street : 6497\n",
      "Hollister AMS : 2178\n",
      "Hollister AMS : 2178\n",
      "Los Gatos : 0\n",
      "Pinnacles NM : 0\n",
      "San Jose - Jackson S : 6662\n",
      "San Jose - Knox Ave : 6698\n",
      "San Martin : 0\n",
      "Fairfield : 0\n",
      "Rio Vista : 5510\n",
      "Rio Vista : 5510\n",
      "Vacaville : 6586\n",
      "Vallejo : 6715\n",
      "Napa - Jefferson St : 0\n",
      "Napa - Napa Valley C : 6499\n",
      "Sonoma Technology Mo : 0\n"
     ]
    }
   ],
   "source": [
    "# get json of all PM2.5 data for the locations of interest; convert to df and combine into one df (pm25_df)\n",
    "# print the number of records found for PM2.5 measurements in 2020 for each location\n",
    "\n",
    "pm25_df = pd.DataFrame(columns=['city', 'coordinates', 'country', 'date', 'location', 'parameter', 'unit', 'value'])\n",
    "\n",
    "for location in bay_area_locations:\n",
    "    loc_params = {\n",
    "        'location': location,\n",
    "        'parameter': 'pm25',\n",
    "        'limit': 10000,\n",
    "        'date_from': '2020-01-01'\n",
    "    }\n",
    "    \n",
    "    loc_resp = requests.get('https://api.openaq.org/v1/measurements', loc_params)\n",
    "    \n",
    "    print(location, ':', loc_resp.json()['meta']['found'])\n",
    "    \n",
    "    loc_df = pd.DataFrame.from_dict(loc_resp.json()['results'])\n",
    "    \n",
    "    pm25_df = pd.concat([pm25_df, loc_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 127936 entries, 0 to 127935\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   index        127936 non-null  int64 \n",
      " 1   city         127936 non-null  object\n",
      " 2   coordinates  127936 non-null  object\n",
      " 3   country      127936 non-null  object\n",
      " 4   date         127936 non-null  object\n",
      " 5   location     127936 non-null  object\n",
      " 6   parameter    127936 non-null  object\n",
      " 7   unit         127936 non-null  object\n",
      " 8   value        127936 non-null  object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 8.8+ MB\n"
     ]
    }
   ],
   "source": [
    "pm25_df.reset_index(inplace=True)\n",
    "pm25_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This df encompasses the PM2.5 measurements for all of our locations of interest for 2020 (up to the present date). There are around 127,000 rows, so it is reasonable to assume that each year's data will be on the same order of magnitude. OpenAQ provides two years of data via their open API, so we will use the same process as above to acquire the previous two years' data. For data prior to that, we will need to query their S3 buckets, which can be done through a distributed query tool like Amazon Athena, Apache Spark, or Google BigQuery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index          False\n",
       "city           False\n",
       "coordinates    False\n",
       "country        False\n",
       "date           False\n",
       "location       False\n",
       "parameter      False\n",
       "unit           False\n",
       "value          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm25_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
