{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAQ Data\n",
    "## Part 1: Compiling Data using the OpenAQ api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can pull the data from the past 2 years from https://docs.openaq.org/\n",
    "\n",
    "# this is a link for a map showing sources and readings: https://openaq.org/#/map?&_k=tqphyo\n",
    "# you can get more details for any location (and download data)\n",
    "# san fran (San Francisco-Oakland-Fremont) data: https://openaq.org/#/location/San%20Francisco?_k=9pwelq\n",
    "# 65k records from 07/2018 - today\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for archived data (06/2015 - 04/2018), you can use https://openaq-data.s3.amazonaws.com/index.html\n",
    "# *about 03/2016 for san fran data\n",
    "# for each city there is a record for each type of reading (o3, co, pm, etc.) noted by the key 'parameter'\n",
    "# the source of the reading is noted by the key 'attribution', which is a dictionary with the name and url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file openAQ already exists.\n"
     ]
    }
   ],
   "source": [
    "# make a folder to save the data\n",
    "! mkdir openAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, text):\n",
    "    folder = 'openAQ'\n",
    "    extension = 'csv'\n",
    "    path = '{}\\{}.{}'.format(folder, filename, extension)\n",
    "    file = open(path, 'w')\n",
    "    file.write(text)\n",
    "    file.close\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_for_range(start_date, end_date):\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    dates = [str(timestamp.date()) for timestamp in date_range]\n",
    "    return dates\n",
    "\n",
    "def is_keywords_in_string(string, keywords):\n",
    "    has_keyword = False\n",
    "    for keyword in keywords:\n",
    "        has_keyword = keyword in string\n",
    "        if has_keyword:\n",
    "            return has_keyword\n",
    "    return has_keyword\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for handling requests\n",
    "def request_openAQ_data(date):\n",
    "    url = 'https://openaq-data.s3.amazonaws.com/{}.csv'.format(date)\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        return response\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes in a date as a string with the format yyyy-mm-dd\n",
    "# filter the results based on lines containing keywords\n",
    "def filter_date_data_for_keywords(date, keywords):\n",
    "    filtered_data = ''\n",
    "    response = request_openAQ_data(date)\n",
    "    if response is not None:\n",
    "        for line in response.iter_lines():\n",
    "            line = str(line, 'utf-8')\n",
    "            if is_keywords_in_string(line, keywords):\n",
    "                filtered_data += line + '\\n' # probably could optimize\n",
    "    return filtered_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_in_date_range(start_date, end_date, keywords):\n",
    "    dates = get_dates_for_range(start_date, end_date)\n",
    "    for date in dates:\n",
    "        print(date)\n",
    "        data = filter_date_data_for_keywords(date, keywords)\n",
    "        if data:\n",
    "            save_file(date, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull all of the daily data and filter the results for san-fran related records and save to a new csv\n",
    "start_date = '2015-06-09'\n",
    "end_date = '2018-04-06'\n",
    "keywords = ['San Fran']\n",
    "filter_data_in_date_range(start_date, end_date, keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Details\n",
    "\n",
    "1. location: The name of the location of the site where the reading was observed\n",
    "2. city: The name of the city\n",
    "3. country: The name of the country\n",
    "4. utc: The time of the reading using the UTC timezone\n",
    "5. local: The time of the reading using the Local timezone\n",
    "6. parameter: The name of the pollutant being observed\n",
    "7. value: The value of the pollutant reading\n",
    "8. unit: The units of the pollutant read\n",
    "9. latitude: The latitude of the site\n",
    "10. longitude: The longitude of the site\n",
    "11. attribution: A dictionary containing the name and url of the source of this data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "This API also provides fairly clean data, with a high degree of geographic specificity. This data will provide the foundation for our analysis, allowing us to determine the severity of fire-related air pollution over time in the Bay Area and thereby to examine its relationship to travel- and mobility-related factors. There is archived daily data from 2016 - 2018. Each file is 270 KB with a total of 200 MB. More recent data (up to 2 years) can be requested with their api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add a small request to show what the data looks like\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
